\documentclass[11pt]{llncs}
\usepackage{makeidx}  % allows for indexgeneration

\usepackage[latin1]{inputenc}
%\usepackage[francais]{babel}
\usepackage{subfigure}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{dsfont}
\usepackage{natbib}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{enumerate}
\usepackage{stmaryrd}
\usepackage{color}
\usepackage{ulem}
\usepackage{float}
\floatstyle{ruled}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{url}
\usepackage[table]{xcolor}

%\newfloat{Algorithm}{thp}{lop}
%\floatname{Algorithm}{Algorithm}
%\newtheorem{de}{Definition}[subsection] % les dÃÂ©finitions et les thÃÂ©orÃÂ©mes sont
\newtheorem{theo}{Theorem}[section]    % numÃÂ©rotÃÂ©s par section
\newtheorem{lem}[theo]{Lemma}
%\newtheorem{cor}[theo]{Corrolary}
%\newtheorem{prop}[theo]{Proposition}

\newcommand{\soft}{\texttt{cghseg}}
\newcommand{\esoft}{\texttt{cghseg }}


\begin{document}

\title{Fast and parallel Algorithm for Population-Based Segmentation of Copy-Number Profiles}

%
\titlerunning{Fast and Parallel segmentation}
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Guillem Rigaill$^1$, Vincent Miele$^2$ and Franck Picard$^2$}
%
\authorrunning{Rigaill et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{G. Rigaill$^1$, V. Miele$^2$, F. Picard$^2$}
%
\institute{Laboratoire Statistique et G\'{e}nome, UMR CNRS 8071, USC INRA Universit\'{e} d'Evry, F-91037 Evry, France \\
\email{rigaill@evry.inra.fr},\\ 
\and
Laboratoire de Biom\'etrie et Biologie Evolutive, UMR CNRS 5558 Universit\'{e} Lyon 1, F-69622, Villeurbanne, France \\
\email{vincent.miele@univ-lyon1.fr},\\ 
\email{franck.picard@univ-lyon1.fr}}

\maketitle   

\begin{abstract}
  Dynamic Programming (DP) based change-point methods have shown very good statistical performance on DNA copy number analysis. However, the quadratic algorithmic complexity of DP has limited their use on high-density arrays or next generation sequencing data. This complexity issue is particularly critical for segmentation and calling of segments, and for the joint segmentation of many different profiles. Our contribution is two-fold. First we provide an at worst linear DP algorithm for segmentation and calling, which allows the use of DP-based segmentation on high-density arrays with a considerably reduced computational cost. For the joint segmentation issue we provide a parallel version of the \texttt{cghseg} package which now allows us to analyze more than 1,000 profiles of length 100,000 within a few hours. Therefore our method and software package are adapted to the next generation of computers (multi-cores) and experiments (very large profiles).
\keywords{DNA Copy Number, Dynamic Programming, Segmentation, Joint Segmentation, Parallel Computing}
\end{abstract}


\newpage
\section{Introduction}

Segmenting heterogeneous signals into regions of common characteristics is often required for biologists that face high dimensional information. This partitioning helps reducing the dimension of the data and provides guidelines for interpretation and further biological investigation. Such methods have been widely applied in Genomics, to unravel DNA sequences structures using base composition \cite{NBM02}, to segment expression profiles \cite{DHG06,NLR09}, and to determine copy-number variations based on array CGH data. Microarray CGH data analysis is certainly the field for which every possible method of segmentation have been tried. Three main categories of methods lead to many developments: Circular Binary Segmentation (CBS, \cite{OVL04}), Hidden Markov Models \cite{MTT06}, and change-point analysis \cite{picard_statistical_2005,picard_2007}. Our purpose in this work is not to compare the different methods for array CGH analysis. Such comparisons have been done elsewhere \cite{WF05}, and extensive reviews now exist on the subject \cite{S08,WPW11}. Our focus is the following: in every comparative study, change-point analysis based on Dynamic Programming (DP) has shown the best performance along with CBS \cite{bams,WF05}. The advantage of DP-based segmentation is that it provides the best global segmentation, $i.e.$ the segmentation that globally minimizes a likelihood criterion, whereas local approaches like CBS only provide local minimizers. Moreover, change-point models can also integrate a calling step that is used to cluster segments that show the same copy-number on average (also called segmentation/clustering) \cite{picard_2007,WKV07}, which has been shown to be of central importance in the segmentation process \cite{WF05}. Unfortunatelly, the algorithmic complexity of DP is proportional to the square of the signal's length, which has hampered the use of such method on high-density arrays for instance. This issue has become even more problematic when dealing with population-based or joint segmentation \cite{PLH11, TPK11,ZSJ10}. Our contribution is two-fold: first we provide a linearized version of the DP algorithm for segmentation/clustering adapting pruning strategies that have recently emerged in the field \cite{killick_optimal_2011,rigaill_2010}. Secondly we deal with the joint segmentation issue by providing a parallel version of existing algorithms implemented in the \texttt{cghseg} package. With the growing availability of multicore computers (from laptops to many-core servers), it has become essential to provide software that use every available ressource. \texttt{R} has been a tremendous plateform for package distribution, and here we provide a new version of the \texttt{cghseg} package, a \textit{next generation package} that is adaptive to available computing power. The performance of \texttt{cghseg} are impressive : segmenting 1,000 profiles of length 100,000 can now be done in few hours, which was impossible before. This makes segmentation models a new exact investigation method that can be used in routine for exploratory as well as deep analysis.

\section{Linearization of Dynamic Programming for Segmentation and Segmentation/Clustering} 

The purpose of segmentation is to partition a signal of $n$ observations $\{Y(t)\}$ into $K$ segments of homogeneous distributional parameter. In this section we deal with the univariate segmentation of one array CGH profile, the case of Joint multivariate segmentation being considered in Section 3. In the following a segment is an interval delimited by two change-points $\tau_k, \tau_{k+1}$ for instance,  with convention $\tau_0 = 1$ and $\tau_K = n+1$, and notation $r_k=\llbracket \tau_k, \tau_{k+1} -1\rrbracket$ stands for segment $k$. A segmentation in $K$ segments is denoted by $m^{(K)} = \{r_1, r_2 \ldots, r_K\}$.

A standard statistical model for segmentation is the detection of changes in the mean of a signal, such that $\forall t \in r_k, \,\, Y(t)\sim \mathcal{N}(\mu_k,\sigma^2).$ In the special case of array CGH, an additional step is the ``calling step'' that is performed by introducing additional (hidden) label variables $\{C(r_k)\}_k$ to cluster each segment into categories such as \texttt{deleted}, \texttt{normal}, \texttt{amplified} (not limited to 3 states). Then the segmentation/clustering model becomes (for fixed $P$ and $p=1,...P$):  $$\forall t \in r_k, \,\, Y(t)|\{C(r_k)=p\} \sim \mathcal{N}(\mu_p,\sigma^2).$$

Once the statistical model has been defined, the main algorithmic challenge lies in the exact determination of the boundaries of segments $\{\tau_k\}$ (and not in the estimation of mean parameters $\mu_k$s or $\mu_p$s depending on the model). A well known solution to this problem is to use Dynamic Programming for a given number of segments $K$ to find the best $global$ segmentation in terms of ``cost'' (to be defined). In this work, we do not deal with the issue of model selection to estimate $K$ and $P$, discussed elsewhere \cite{picard_statistical_2005,ZS07}. To perform Dynamic Programming, we need to define the ``unit'' cost of a generic segment $r = \llbracket t_1, t_2 \rrbracket$, which is given by minus the local log-likelihood calculated on $r$:
$$ C_{(r)}^{(1)} = 
\begin{cases}
\sum_{t \in r} (y(t) - \mu_r)^2/2 \sigma^2 \text{, for segmentation in the mean}  \\
-\log\left(\sum_p \pi_p \exp \left\{ - \sum_{t \in r} (y(t) - \mu_p)^2 / 2 \sigma^2 \right\}\right) \text{, for seg/clust}.
\end{cases}
$$ 
A main difference between the two models lies in the estimation of the mean parameters. In the case of segmentation in the mean, parameters $\{\mu_k\}_k$ can be estimated directly by the empirical means of segments while computing the position of the breaks. In the case of segmentation/clustering, parameters $\{\mu_p\}_p$ are common across segments. Consequently, they are fixed while computing breakpoint coordinates, and they are estimated iteratively by using an EM-algorithm, leading to a so-called DP-EM algorithm \cite{picard_2007}. In the case of segmentation/clustering, we propose to simplify the cost function by using a \textit{classification cost function}, an approximation denoted by $\widetilde{C}_{(r)}^{(1)}$ which consists in focusing on the dominant term within the sum over $P$ exponentials:
\begin{equation}
\label{Eq:approx}
\widetilde{C}_{(r)}^{(1)}= \min_p \left\{ \sum_{t \in r}\frac{(y(t) - \mu_p)^2} {2\sigma^2 } + \log(\pi_p)\right\}.
\end{equation}

Since the purpose is to find the global minimum of the total cost function into $K$ segments, we also introduce the set of all segmentations of a given segment $r$ into $K$ segments such that $\mathcal{M}^{(K)}_{(r)} = \mathcal{M}^{(K)}_{t_1, t_2}$. Then the optimal cost of a segmentation of $r$ into $K$ segments and its associated optimal segmentation are defined as:
$$
C_{(r)}^{(K)} = \min_{m \in \mathcal{M}^{(K)}_{(r)}} \left\{ \sum_{r \in m} C^{(1)}_{(r)} \right\}, 
\text{ and }
\widehat{m}_{(r)}^{(K)} = \underset{m \in \mathcal{M}^{(K)}_{(r)}}{\operatorname{argmin}} \left\{ \sum_{r \in m} C^{(1)}_{(r)} \right\}.
$$
Similarly, when Approximation $\widetilde{C}_{(r)}^{(1)}$ is used (Equation (\ref{Eq:approx})), we use notations $\widetilde{C}_{(r)}^{(K)}$ and $\widetilde{m}_{(r)}^{(K)}$. When the cost of a segmentation is segment additive (which is the case in both models), a $\mathcal{O}(Kn^2)$ Dynamic Programming algorithm can be built to recover the best exact segmentation (into $1$ to $K$ segments).

\subsection{Original Dynamic Programming Algorithm for Segmentation}

A basic statement is that the cost of a given segmentation is the sum of the cost of its segments. Thus the Bellman optimality principal holds and we have: $ C_{1, t}^{(k+1)} = \min_{\tau \leq t} \{ C_{1, \tau -1}^{(k)}  + C_{\tau, t}^{(1)} \} .$
%More generally for any $k_1 (>0)$ and $k_2 (> 0)$   we have %$$ C_{1, t}^{(k_1 + k_2)} = min_{\tau \leq t} \{ C_{1, \tau -1}^{(k_1)}  + C_{\tau, t}^{(k_2)} \} $$
Using this update rule  a Dynamic Programming algorithm can be built to recover all $ C_{1, t}^{(k+1)}$ for all $t \leq n$ and $k \leq K$.
This can be done using Algorithm \ref{algo:DPA2} for instance. 
For simplicity we did not include the initialization of all $C^{(1)}_{1,t}$ for $t \leq n$ and of all $C^{(k)}_{1,t}$ for $k \leq K$ and $t < k$.
All $C^{(k)}_{1,t}$ are initialized as $+\infty$ and $C^{(1)}_{1,t}$ are initialized using their definition.
This algorithm assumes that all $C_{t_1, t_2}^{(1)}$ have been pre-computed and stored (in 
a $n$ by $n$ matrix) or that they can be efficiently computed on the fly, which is the case for every models we consider here. At step $k, t$ of Algorithm \ref{algo:DPA2}  $\mathcal{O}(t)$ basic operations are performed. If we sum these for all $k < K$ and $t < n$  we see that the algorithm has a $\mathcal{O}(Kn^2)$ time complexity. This $n^2$ factor is the main reason why Dynamic Programming can be prohibitive to use on large signals (like SNP arrays for instance).
\begin{algorithm}
  \caption{Standard DP algorithm}\label{algo:DPA2}
  \begin{algorithmic}
    \State \textbf{Input}: $Y(t)$ a profile of $n$ observations, $K$ an integer
    \State \textbf{Output}: $C^{(k)}_{1,t}$ in $\mathbb{R}$ and $M^{(k)}_{1,t}$ in $\mathbb{N}$ for all $k \leq K$ and $t \leq n$
    %I'd like to do \hline here!
    \For{$t \in \llbracket 1, n \rrbracket$}
         \State $C^{(1)}_{1,t} = C_{1t}$ ; $M^{(1)}_{1,t} = 0$  
    \EndFor
   %\textbf{Main} & \\
   \For{$k \in \llbracket 2, \min(t, K) \rrbracket$}
      \For{$t \in \llbracket 1, n \rrbracket$} 
     	\State $C^{(k)}_{1,t} = \underset{k-1 \leq \tau \leq t-1}{\min} \{ C^{(k-1)}_{1,\tau}+ C_{(\tau+1)t}^{(1)} \}$ ; $M^{(k)}_{1,t} =\underset{k-1 \leq \tau \leq t-1}{\operatorname{argmin}} \{ C^{(k-1)}_{1,\tau}+ C_{(\tau+1)t}^{(1)} \}$ 
        \EndFor   
     \EndFor
  \end{algorithmic}
\end{algorithm}

\subsection{A Linear Dynamic Programming Algorithm for the Classification Cost Function}

An important consequence of using the classification cost $\widetilde{C}_{(r)}^{(1)}$ rather than ${C}_{(r)}^{(1)}$ is that the set of canditate segmentations 
can be pruned efficently. The idea of pruning the set of candidate segmentations
is not new and was proposed for other cost functions  \cite{killick_optimal_2011, rigaill_2010}.
In these two algorithms the pruning step usually allows for an important speed up and the average time complexity is for many signal in $\mathcal{O}(n)$ or  $\mathcal{O}(n\log(n))$. Nonetheless, in both cases, the worst case is quadratic with respect to $n$.
In the case of the classification cost however the pruning step is particularly 
efficient %and given that the set of possible values for $\mu_p$ is finite 
and we can guarantee that the time and space complexity of the algorithm are at worst in $\mathcal{O}(KPn)$. 
Furthermore, we can also guarantee that the cost of the recovered segmentation $\widetilde{m}_{(r)}^{(K)}$ is at worth within $K \log(p)$ of the optimal segmentation $\widehat{m}_{(r)}^{(K)}$ (see Theorem \ref{theo:theoapprox}). \\
\indent Before we describe the algorithm we need to define some new notations. We define the approximate cost of a segment knowing its mean $\mu$ as 
$ \widetilde{C}_{(r)}^{(1)}   (\mu) = \left\{ \sum_{t \in r} (y(t)- \mu)^2  /2 \sigma^2  - \log(\pi_p) \right\}.$ Using this notation we can rewrite the classification cost of a segment $r = \llbracket t_1, t_2 \rrbracket$ as $\widetilde{C}_{t_1, t_2}^{(1)}  = \min_p \{  \widetilde{C}_{t_1, t_2}^{(1)}  (\mu_p) \},$ with $\widetilde{C}_{t_1, t_2}^{(1)}  (\mu)$ being point additive in the sense that $\widetilde{C}_{t_1, t_2}^{(1)}  (\mu) = \underset{ t_1 \leq t \leq t_2}{\sum} \widetilde{C}_{t, t}^{(1)}  (\mu).$ Then we define the cost of the best segmentation knowing that the mean of the last segment is $\mu$ as:
$$\widetilde{C}_{1, t}^{(K)}(\mu) = \underset{{m \in \mathcal{M}^{(K)}_{(1, t)}}}{\min} \left\{ \sum_{k < K-1}  \widetilde{C}^{(1)}_{(r_k)}  + \widetilde{C}^{(1)}_{(r_K)}(\mu) \right\}.$$
Using this notation we get that $ \widetilde{C}_{1, t}^{(k)}  = \underset{p < P}{\min} \left\{ \widetilde{C}_{1, t}^{(k)}(\mu_p) \right\}$, and if we know every $C_{1, t}^{(k)}(\mu_p)$ at step $t$ for all $p< P$, we straightfowardly get $C_{1, t}^{(k)}$ in $\mathcal{O}(p)$. As $\widetilde{C}_{t_1, t_2} (\mu)$ is point additive 
updating $\widetilde{C}_{t_1, t_2} (\mu)$ is easy and can be done efficiently using the following theorem.

%the set of potential candidate segmentations can be pruned efficiently as already described \cite{rigaill_2010}. Moreover the set of $\mu_p$ being finite, the 
%pruning step is further simplified and can be done using the following theorem.

\begin{theo}
$ \widetilde{C}_{1, t+1}^{(k)}(\mu) = \min \left\{ \widetilde{C}_{1, t}^{(k)}(\mu),  \widetilde{C}_{1, t}^{(k-1)} \right\} +  \widetilde{C}_{t+1, t+1}^{(1)}(\mu) $
\end{theo}

\paragraph{Proof. } Let us first notice that: 
$ \widetilde{C}_{1, t+1}^{(k)}(\mu) =  \underset{{\tau < t+1} }{\min}\left\{  \widetilde{C}_{(1,\tau)}^{(k-1)}  +  \widetilde{C}_{(\tau+1, t)}^{(1)}(\mu) \right\}
.$
Using the defintion of $\widetilde{C}_{1, t}^{(k)}(\mu) $ we get that:

$$ \widetilde{C}_{1, t}^{(k)}(\mu) + \widetilde{C}_{(t+1, t+1)}^{(1)}(\mu)=
 \underset{{\tau < t}}{\min} \left(  \widetilde{C}_{(1,\tau)}^{(k-1)} \right) + \widetilde{C}_{(t+1, t+1)}^{(1)}(\mu)$$
%\begin{eqnarray*}
% \widetilde{C}_{1, t+1}^{(k)}(\mu) = & \min \left\{ \ \underset{{\tau < t}}{\min} \left(  \widetilde{C}_{(1,\tau)}^{(k-1)}  +  \widetilde{C}_{(\tau+1, t)}^{(1)}(\mu) \right), \quad  \widetilde{C}_{(1,t)}^{(k-1)}  \right\} +  \ \widetilde{C}_{(t+1, t+1)}^{(1)}(\mu).
%\end{eqnarray*}
\noindent From this the theorem follows. $\blacksquare$ \\

Using this theorem, knowing $\widetilde{C}_{1, t}^{(k)}(\mu)$ and $\widetilde{C}_{1, t}^{(k-1)}$ we get $\widetilde{C}_{1, t+1}^{(k)}(\mu)$ in $\mathcal{O}(1)$
and we derive Algorithm \ref{algo:DPALinear} for the DP step of the DP-EM algorithm \cite{picard_2007}. For simplicity we did not include the initialization of $\widetilde{C}^{(1)}_{1,t}$ for $t < n$ and $\widetilde{C}^{(k)}_{1,1}(\mu_p)$.
All $\widetilde{C}^{(k)}_{1,1}(\mu_p)$ are initialized as $+\infty$ and $\widetilde{C}^{(1)}_{1,t}$ are initialized using their definition. At step $k, t$ of Algorithm \ref{algo:DPALinear}  $\mathcal{O}(P)$ basic operations are performed. If we sum these for all $k < K$ and $t < n$  we straighfowardly see that the algorithm has an $\mathcal{O}(KPn)$ time complexity.
\begin{algorithm}
\begin{algorithmic}
\caption{Linear DP algorithm for the classification cost}\label{algo:DPALinear}
    \State \textbf{Input}: $Y(t)$ a profile of $n$ observations, $K$ an integer 
 \State \textbf{Input}: $C_{t_1,t_2}$ cost of the segments $\rrbracket t_1, t_2 \rrbracket$  for all $(t_1, t_2) \in \llbracket1, n \rrbracket^2$
   \State \textbf{Output}: $C^{(k)}_{1,t}$ in $\mathbb{R}$ and $M^{(k)}_{1,t}$ in $\mathbb{N}$ for all $k \leq K$ and $t \leq n$ 
%    \State $M^{(k)}_{1,t}$ in $\mathbb{N}$ for all $k \leq K$ and $t \leq n$ 
   %& $M^{(k)}_{1,t}$ in $\mathbb{N}$ for all $k \times K$ and $t \leq n$ \\
  %  \textbf{Initialize}& \\
  % & \textbf{For} $t \in \llbracket 1, n \rrbracket$ \\
  % & \begin{tabular}{ll}
  %   & $C^{(1)}_{1,t} = C_{1t}$ \\
    % & $M^{(1)}{1,t} = 0$  
  %  \end{tabular} \\
  % & \textbf{End For} \\

      \For{ $k \in \llbracket 2, \min(t, K) \rrbracket$}
       \For{$t \in \llbracket 1, n \rrbracket$}
        \For {$p \in \llbracket 1, P \rrbracket$}  
           \State $C^{(k)}_{1,t}(\mu_p) = \min \{ C^{(k)}_{1,t-1}(\mu_p), C^{(k-1)}_{1,t-1} \}$ ; 
           \State $M^{(k)}_{1,t}(\mu_p) = \operatorname{argmin} \{C^{(k)}_{1,t-1}(\mu_p), C^{(k-1)}_{1,t-1} \}$ 
          \EndFor        
           \State $C^{(k)}_{1,t} = \underset{p}{\min} \{ C^{(k)}_{1,t}(\mu_p) \}$ ; $p^* = \underset{p}{\operatorname{argmin}} \{ C^{(k)}_{1,t}(\mu_p) \}$ ; $M^{(k)}_{1,t} = M^{(k)}_{1,t}( \mu_{{p^*}}) $ 
         \EndFor
    \EndFor
  \end{algorithmic}
\end{algorithm}

\subsection{A Bound on the Quality of the Approximation }
Using the approximation defined in Equation \ref{Eq:approx} we can guarantee the quality of the obtained segmentation using the following theorem.
\begin{theo}
Using approximation defined in Equation \ref{Eq:approx} we have for all segments $R$ and $K$
$$\widetilde{C}_{(R)}^{(K)} - K \log(p) 
\ \leq \ 
\underset{r \in \widehat{m}_{(R)}^{(K)}}{\operatorname{\sum}} \widetilde{C}_{(R)}^{(K)} - K \log(p) 
\ \leq \ 
{C}_{(R)}^{(K)} 
\ \leq \ 
\underset{r \in \widetilde{m}_{(R)}^{(K)}}{\operatorname{\sum}} {C}_{(R)}^{(K)} 
\ \leq \ 
\widetilde{C}_{(R)}^{(K)}.$$
\label{theo:theoapprox}
\end{theo}
\vspace{-1cm}
\noindent {\textit{Proof.} 
We have ${C}_{(r)}^{(1)} = -log(\sum_p \exp \{-\widetilde{C}_{(r)}^{(1)}(\mu_p) \})$ and  $ \widetilde{C}_{(r)}^{(1)} \geq \widetilde{C}_{(r)}^{(1)}(\mu_p).$
%Since $$ {C}_{(r)}^{(1)} = \widetilde{C}_{(r)}^{(1)} - \log\left(\sum_p \pi_p \exp \left\{ -\sum_{t \in r} (y(t)- \mu_%p)^2 / 2 \sigma^2 +\widetilde{C}_{(r)}^{(1)} \right\}\right),$$
%$$\text{and }-\log(p) \ \leq \ - \log\left(\sum_p \pi_p \exp \left\{ -\sum_{t \in r} (y(t) - \mu_p)^2 / 2 \sigma^2 +\w%idetilde{C}_{(r)}^{(1)} \right\}\right)\ \leq \  
%0,$$
From this we get that : $\forall r,\, \widetilde{C}_{(r)}^{(1)} -\log(p) \leq {C}_{(r)}^{(1)} \leq \widetilde{C}_{(r)}^{(1)} \label{equation:bound1}$, which gives, along with the definition of ${C}_{(R)}^{(K)}$: 

\begin{eqnarray} 
\forall m \in \mathcal{M}^{(K)}_{(R)}, \,
\widetilde{C}_{(R)}^{(K)} - K\log(p) \leq \sum_{r \in m} \widetilde{C}_{(r)}^{(1)} - K \log(p) \leq   \sum_{r \in m} {C}_{(r)}^{(1)}.
\label{equation:boundproof1}
\end{eqnarray}
Similarly we get:
\begin{eqnarray} 
\forall m \in \mathcal{M}^{(K)}_{(R)}, \, {C}_{(R)}^{(K)} \leq \sum_{r \in m} {C}_{(r)}^{(1)} \leq   \sum_{r \in m} \widetilde{C}_{(r)}^{(1)}.
\label{equation:boundproof2}
\end{eqnarray}

Applying Equation \ref{equation:boundproof1} to $m = \widehat{m}_{t_1, t_2}^{(K)}$ and then Equation \ref{equation:boundproof2} to $m =  \widetilde{m}_{t_1, t_2}^{(K)}$ we get the theorem. $\blacksquare$

\section{Joint Segmentation and Parallelization of the Algorithm}

Joint segmentation arises when more than one profile should be segmented jointly. We make the distinction between simultaneous segmentation, where all breakpoints are the sames across profiles, with joint segmentation where all profiles have their own specific breakpoints, but may share some characteristics, like the same noise, the same biases, the same values for the mean of segments that share the same copy numbers ($i.e.$ parameters $\mu_p$). When segmenting $I$ profiles jointly, a typical model is: 
$$
\forall i \in [1,I],\,\, \forall t \in r_k^i, \,\, Y_i(t)|\{C(r_k^i)=p\} \sim \mathcal{N}(\mu_p+b(t),\sigma^2),
$$
where ${r_k^i}$ stands for segment $k$ of profile $i$, and where $b(t)$ is a bias function that depends on the position (like the wave effect \cite{PLH11}). Then the segmentation of profile $i$ into $K_i$ segments is denoted by $m_{i}^{K_i}=\{r_1^i,...r_{K_i}^i\}$, and the $global$ segmentation into $K$ segments is denoted by $m_K=\{m_1^{K_1}, ..., m_{I}^{K_I}\}$, with $K=\sum_{i=1}^I K_i$.

Joint segmentation presents an additional algorithmic challenge. When each $K_i$ is known, the best segmentation for each profile $\widehat{m}_{i}^{K_i}$ can be found independently and therefore computed in parallel using Algorithm 1 or 2. Then the additional step, which remains sequential, is to determine $i)$ the common parameters across profiles ($\{\mu_p\}, b(t)$) and $ii)$ the best combination of $\{\widehat{K}_i\}$ that provides the best joint segmentation for a given total number of segments $K$ \cite{PLH11}. Dynamic Programming has also been shown to provide an exact solution to problem $ii)$ in $\mathcal{O}(I^3 K_{\max}^2)$, where $K_{\max}$ is the maximum number of segments to be put in each profile. Consequently, DP-based joint segmentation alternates between parallel steps (computation of individual segmentations) and sequencial steps (estimation of common parameters and determination of the best combination of individual segmentations), as shown in Algorithm \ref{algo:parallelDP}.
\begin{algorithm}
\begin{algorithmic}
\caption{Parallel Algorithm for Joint segmentation}\label{algo:parallelDP}
\State \textbf{Input}: $\{Y_i(t)\}$, $I$ profiles of $n$ observations, $K$ an integer, $\{\widehat{\mu}_p^0\}$ starting values for common parameters
\State \textbf{Input}: $C_{t_1,t_2}^i$ cost of the segments $\rrbracket t_1, t_2 \rrbracket$ for profile $i$ for all $(t_1, t_2) \in \llbracket1, n \rrbracket^2$
\State \textbf{Output}: $\{\widehat{K}_1,..., \widehat{K}_I\}$, $C^{(\widehat{K}_i)}_{1,t}$ in $\mathbb{R}$ for all $i \leq I$, $k \leq K_i$ and $t \leq n$, $\{\widehat{\mu}_p\}$, $\widehat{b}(t)$ 
\While{not convergence}
\For{$i \in \{1,...I\}$} {\bf in parallel}
\State compute $C^{(k)}_{1,t}$  $\forall k \leq K$ and $t \leq n$ with Algorithm \ref{algo:DPALinear}
\EndFor
\State update $\{\widehat{\mu}_p\}, \widehat{b}(t)$
\State compute $\{\widehat{K}_1,..., \widehat{K}_I\}$ with {\bf sequential} DP \cite{PLH11}
\EndWhile
  \end{algorithmic}
\end{algorithm}

\section{Correctness, Computational Footprint and Scalability}

All the presented algorithms are available into the \esoft \texttt{R}-package\footnote{\url{http://cran.r-project.org/web/packages/cghseg}} , which now relies on the \texttt{parallel} \texttt{R}-package and on the shared memory programming standard \texttt{openMP} in the \texttt{C++} sections. The new package is now designed to be executed in parallel on multiprocessor architectures. 

We recall that the statistical performance of the model have been discussed elsewhere \cite{PLH11}, so that our focus here is computational only. Consequently, we use a previously published simulation scheme to generate the data \cite{PLH11,POA09} with $n=20,000$ observations per profile, and a number of profiles of $256, 512$ and $1024$. The average number of segments is set to $10$ for each profile, and the Signal to Noise Ratio is set to $5$ (which corresponds to  moderately easy configurations \cite{PLH11}). 

We first check the correctness of our method, by verifying that the linearization approximations (with and without calling) give the same statistical performance compared with the non-linearized version. This is shown in Table \ref{Tab:stat}), as the performance are identical over 50 replicates.

Then we assess the effectiveness of the parallel implementation by running \texttt{cghseg} on an increasing number of cores $(1,2,4,16,32,48)$. When dealing with parallelized codes, the Amdahl's law \cite{AG67} provides the expected theoretical speedup with respect to the time proportion of the sequential part and the number of cores. The speedup of \texttt{cghseg} exactly follows the Amdahl's law when the number of profiles is high (Fig.\ref{figspeedup}, dashed lines), which demonstrates the quality of our implementation as the Amdahl's law constitutes the best possible speedup. However, we observe an unexpected moderate speedup decrease for a lower number of profiles. This is due to overheads associated with the use of the \texttt{parallel} \texttt{R}-package, overheads which become negligible when the number of profiles is high. Still, the execution time of configurations with $256$ profiles is $6$ minutes on average using $48$ cores, which remains excellent. 

While the main interest of \esoft lies in the quality of its results, the associated computational expense is affordable even for very large datasets. As a benchmark, we simulated datasets with $1024$ profiles of length $100,000$ which corresponds to the up-to-date limits of the available datasets of SNP-arrays for instance. When joint segmentation is performed along with calling, \texttt{cghseg} required $4$h on average on $48$ cores (see Table \ref{tabtime}), which is very reasonable considering the size of the dataset. Lastly, due to the {\it copy-on-write} mechanism  of the \texttt{parallel} \texttt{R}-package that avoids memory copy between processes, and thanks to the shared memory efficiency provided by \texttt{openMP}, the memory needs of \texttt{cghseg} only correspond to the dataset under study (see Table \ref{tabtime}). Therefore, our method and software package are adapated to the next generation of computers (many cores) and experiments (large profiles).

\begin{table}[ht]
\begin{center}
\begin{small}
\begin{tabular}{|c|rr|rr|rr|rr|}
  \hline
SNR  &  $\text{CI}_{\text{new}}(\widehat{\text{K}})$ & $\text{CI}_{\text{old}}(\widehat{\text{K}})$ & $ \text{CI}_{\text{new}}(\hat{\mu})$ & $\text{CI}_{\text{old}}(\hat{\mu})$ & $\text{CI}_{\text{new}}(\text{fdr})$ & $\text{CI}_{\text{old}}(\text{fdr})$   
     & $\text{CI}_{\text{new}}(\text{fnr})$ & $\text{CI}_{\text{old}}(\text{fnr})$  \\
  \hline
  1   & [0.90;1.02] & [0.90;1.02] & [0.14;0.16] & [0.14;0.17] & [0.42;0.45] & [0.42;0.45] & [0.59;0.62] & [0.59;0.62] \\ 
  5   & [0.36;0.44] & [0.37;0.45] & [0.05;0.06] & [0.05;0.06] & [0.15;0.20] & [0.17;0.22] & [0.21;0.26] & [0.23;0.28] \\ 
  10  & [0.22;0.30] & [0.23;0.30] & [0.03;0.03] & [0.03;0.03] & [0.06;0.10] & [0.07;0.11] & [0.08;0.14] & [0.09;0.14] \\ 
  15  & [0.17;0.22] & [0.17;0.23] & [0.02;0.02] & [0.02;0.02] & [0.02;0.06] & [0.03;0.07] & [0.03;0.08] & [0.03;0.09] \\ 
  20  & [0.15;0.20] & [0.15;0.21] & [0.01;0.02] & [0.01;0.02] & [0.01;0.05] & [0.01;0.05] & [0.01;0.06] & [0.01;0.07] \\ 
   \hline
\end{tabular}
\end{small}
\end{center}
\caption{Performance comparison between the non linearized and non parallel (``old'') and the linearized-parallel (``new'') version. Confidence intervals are given over 50 replicates. Statistical performance is assessed through the precision of the estimation of the number of segments $\hat{K}$ and of the mean level of each segment ($\hat{\mu}$), the False Discovery and False Negative Rates for breakpoint detection.\label{Tab:stat}} 
\end{table}

\begin{figure}[h!]
  \begin{center}
    \includegraphics[scale=0.45,angle=270]{figures/speedup.ps}
    \caption{Average speedup of \esoft observed  on simulated datasets for a varying number of profiles and available cores (plain lines). Amdahl's law is empirically adjusted to the computing results with $256,512$ and $1024$ profiles (dashed lines, with $\alpha=0.2,1.2,4.5\%$ respectively). The theoretical speedup is given by $1/(\alpha+(1-\alpha)/c)$, where $\alpha$ is the time proportion of the sequential part and $c$ the number of cores. Points correspond to averages over 5 replicates. Run on a quadri-12 cores Opteron 2.2 GHz, 256 Gb RAM.}
    \label{figspeedup}
  \end{center}
\end{figure}

\begin{table}[h!]
  \begin{center}
    %\scriptsize{
    %\scalebox{0.89}{
    %\begin{tabular}{>{\columncolor{lightgray}}c|ccc|ccc}
    \begin{tabular}{|c|ccc|ccc|}
      \hline
      $I$ (number of profiles) &  \multicolumn{3}{c}{$20,000$} & \multicolumn{3}{c|}{$100,000$} \\
      $n$ (observations/profile) & 256 & 512 & 1024 & 256 & 512 & 1024\\
      \hline
      Average CPU time (min) & 6 & 15 & 54     & 31 & 70 & 253 \\
      Memory usage (Gb)      & 0.4 & 0.8 & 1.8    &  1.7 & 3.7 & 7.9 \\
      \hline
    \end{tabular}
    %}
    \caption{Average computational requirements of \esoft estimated on simulated datasets for a varying number of profiles and observations, computed on $48$ cores. Results correspond to averages over 5 replicates. Run on a quadri-12 cores Opteron 2.2 GHz, 256 Gb RAM.}
    \label{tabtime}
  \end{center}
\end{table}

\bibliographystyle{plain}
\bibliography{biblio}


\end{document}


\section{Linearized Dynamic Programming to Segment Individual Profiles}
\subsection{Background / Notations}
We consider a signal of length $n$, $\{X_i\}_{i \leq n}$ with $X_i \in \mathbf{R}$.
A segment $r$ is delimited by two change-points and is denoted
 $r = \llbracket t_1, t_2 \rrbracket$.
We define the set of all segmentations of a segment $r$ in $K$ segments as $\mathcal{M}^{(K)}_{(r)} = \mathcal{M}^{(K)}_{t_1, t_2}$.
The $k$-th change of segmentation $m$ in $K$ segments will be denoted by $\tau_k$ with the convention that $\tau_0 = 1$ and $\tau_K = n+1$.
The $k-th$ segment is denoted by $r_k$.
For a given segmentation in $K$ segments we have, $m = \{r_1, r_2 \ldots, r_K\}$ and for all $k \leq K$,  $r_k = \llbracket \tau_k, \tau_{k+1} -1\rrbracket$.

Using the notations of seg-clust (\cite{picard_2007}) the cost of a given segment $r = \llbracket t_1, t_2 \rrbracket$ is given by minus the log-likelihood :
$$ C_{t_1, t_2}^{(1)} =C_{(r)}^{(1)} = -\log\left(\sum_p \pi_p exp\left\{ - \frac{\sum_{i \in r} (x_i - \mu_p)^2 }{ 2 \sigma^2} \right\}\right)$$ 
The optimal cost in $K$ segments from $r = \llbracket t_1, t_2 \rrbracket$ is defined as :
$$C_{t_1, t_2}^{(K)} = C_{(r)}^{(K)} = \min_{m \in \mathcal{M}^{(K)}_{(r)}} \left\{ \sum_{r \in m} C^{(1)}_{(r)} \right\}.$$
The corresponding  optimal segmentation is defined as :
$$\widehat{m}_{t_1, t_2}^{(K)} = \widehat{m}_{(r)}^{(K)} = \underset{m \in \mathcal{M}^{(K)}_{(r)}}{\operatorname{argmin}} \left\{ \sum_{r \in m} C^{(1)}_{(r)} \right\}.$$
The cost of a segmentation is segment additive. Using this segment additivity one can built an $\Theta(Kn^2)$ dynamic programming algorithm to recover the best segmentations (in $1$ to $K$ segments).

Here we propose to approximate this cost in a K-mean fashion
More specifically we propose two possible approximations, the first is:
\begin{eqnarray} \widetilde{C}_{(r)}^{(1)} \approx  \min_p \left\{ \sum_{i \in r}\frac{(x_i - \mu_p)^2} {2\sigma^2 }\right\}
\label{equation:approximation2}\end{eqnarray} 
and the second is 
\begin{eqnarray} \widetilde{C}_{(r)}^{(1)} \approx  \min_p \left\{ \sum_{i \in r}\frac{(x_i - \mu_p)^2} {2\sigma^2 } + \log(\pi_p)\right\} .
\label{equation:approximation1}\end{eqnarray} 
In both cases, $\widetilde{C}_{(r)}^{(K)}$ and $\widetilde{m}_{(r)}^{(K)}$ are defined as:
$$ \widetilde{C}_{(r)}^{(K)} =\min_{m \in \mathcal{M}^{(K)}_{(r)}} \left\{ \sum_{r \in m} \widetilde{C}^{(1)}_{(r)} \right\}, \qquad \text{and} \qquad
 \widetilde{m}_{(r)}^{(K)} = \underset{m \in \mathcal{M}^{(K)}_{(r)}}{\operatorname{argmin}} \left\{ \sum_{r \in m} \widetilde{C}^{(1)}_{(r)} \right\}.$$


%We also have that for all $\tau, \tau' \leq t$ such that
%$$  \widetilde{C}_{(1,\tau)}^{(k)}  +  \widetilde{C}_{(\tau+1, t)}^{(1)}(\mu) \leq  \widetilde{C}_{(1,\tau')}^{(k)}  +  \widetilde{C}_{(\tau'+1, t)}^{(1)}(\mu) $$
% then for any $t' > t$ we have :
%$$  \widetilde{C}_{(1,\tau)}^{(k)}  +  \widetilde{C}_{(\tau+1, t)}^{(1)}(\mu) + \widetilde{C}_{(t+1, t')}^{(1)} \leq  \widetilde{C}_{(1,\tau')}^{(k)}  +  \widetilde{C}_{(\tau'+1, t)}^{(1)}(\mu)  + \widetilde{C}_{(t+1, t')}^{(1)}. $$
%Thus we get that $\forall \tau < t $ : 
%$$  \widetilde{C}_{1, t}^{(k)}(\mu) + \widetilde{C}_{(t+1, t+1)}^{(1)} \leq  \widetilde{C}_{(1,\tau)}^{(k)} + \widetilde{C}_{(\tau+1, t+1)}^{(1)}(\mu).$$
%Thus 
%$$ \min_{\tau < t} \left(  \widetilde{C}_{(1,\tau)}^{k-1}  +  \widetilde{C}_{(\tau+1, t)}^{(1)}(\mu) \right) $$
